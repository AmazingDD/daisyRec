{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is a detail introduction for anyone interested in this repository(basic knowledge for recommender system is essential). DaisyRec aims to design a toy bricks for every details in recommender system. Although `main.py` provided a quik interface for users to get results, I don't think all of them wanna that, so this tutorial will split `main.py` and generate visible results for you to have a straight view over daisy.\n",
    "\n",
    "This tutorial will take movielens-100k dataset as a example, then make recommendation list step by step. Hope it will be helpful for you. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T05:29:02.306918Z",
     "start_time": "2020-08-20T05:29:01.490493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading [ml-100k]-[5core] dataset\n"
     ]
    }
   ],
   "source": [
    "from daisy.utils.loader import load_rate\n",
    "\n",
    "df, user_num, item_num = load_rate('ml-100k', '5core', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more details for `load_rate` function could be reviewed by typing `?load_rate` in the cell below, and the other functions could also show the details via this method, I'll then ommit this operation in the following part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T05:29:02.341802Z",
     "start_time": "2020-08-20T05:29:02.307893Z"
    }
   },
   "outputs": [],
   "source": [
    "?load_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T05:29:02.349781Z",
     "start_time": "2020-08-20T05:29:02.342799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1349)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_num, item_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T05:29:02.360751Z",
     "start_time": "2020-08-20T05:29:02.350779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>195</td>\n",
       "      <td>240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304</td>\n",
       "      <td>240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>886307828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>883268170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>233</td>\n",
       "      <td>240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>891033261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>875747190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  rating  timestamp\n",
       "0   195   240     1.0  881250949\n",
       "1   304   240     1.0  886307828\n",
       "2     5   240     1.0  883268170\n",
       "3   233   240     1.0  891033261\n",
       "4    62   240     1.0  875747190"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find that all user ID and item ID have already been categorized. Now, after loading the original experiment data, we need split it into training set and test set. Here we user fold-out strategy(also known as split-by-ration) and extract 20% data as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T05:29:02.845737Z",
     "start_time": "2020-08-20T05:29:02.361749Z"
    }
   },
   "outputs": [],
   "source": [
    "from daisy.utils.loader import get_ur\n",
    "from daisy.utils.splitter import split_test\n",
    "\n",
    "train_set, test_set = split_test(df, 'fo', .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further KPI calculation, we need figure out the ground truth for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T05:29:08.212948Z",
     "start_time": "2020-08-20T05:29:02.846711Z"
    }
   },
   "outputs": [],
   "source": [
    "# get ground truth\n",
    "test_ur = get_ur(test_set)\n",
    "total_train_ur = get_ur(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T05:29:08.215940Z",
     "start_time": "2020-08-20T05:29:08.213945Z"
    }
   },
   "outputs": [],
   "source": [
    "# tmp = list(test_ur.keys())[0]\n",
    "# tmp, test_ur[tmp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Algorithm\n",
    "\n",
    "Taking BPR-MF as an example, we should firstly sample some negative samlpes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T05:29:21.789430Z",
     "start_time": "2020-08-20T05:29:08.216937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish negative samplings, sample number is 317716......\n"
     ]
    }
   ],
   "source": [
    "from daisy.utils.sampler import Sampler\n",
    "\n",
    "sampler = Sampler(\n",
    "    user_num, \n",
    "    item_num, \n",
    "    num_ng=4, \n",
    "    sample_method='uniform', \n",
    "    sample_ratio=1\n",
    ")\n",
    "neg_set = sampler.transform(train_set, is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`neg_set` is a 2-dimension list whose element is a [user, item, tag, negative set] list\n",
    "\n",
    "after finish negative sampling, we need initialize the recommender class, as we all know, BPR-MF is a pair-wise ranking issue, so we take `PairMF` in `daisy.model.pair.MFRecommender` as the target method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T05:29:22.081649Z",
     "start_time": "2020-08-20T05:29:21.790427Z"
    }
   },
   "outputs": [],
   "source": [
    "from daisy.model.pair.MFRecommender import PairMF\n",
    "\n",
    "model = PairMF(\n",
    "    user_num, \n",
    "    item_num,\n",
    "    factors=15,\n",
    "    epochs=50,\n",
    "    lr=0.01,\n",
    "    reg_1=0.,\n",
    "    reg_2=0.01,\n",
    "    loss_type='BPR',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T05:25:39.205696Z",
     "start_time": "2020-08-20T05:25:39.201706Z"
    }
   },
   "source": [
    "the following code is just similar to the pytorch coding style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T05:39:10.113587Z",
     "start_time": "2020-08-20T05:29:22.082646Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 001]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 103.92it/s, loss=13.9]\n",
      "[Epoch 002]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 104.74it/s, loss=13.9]\n",
      "[Epoch 003]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 106.79it/s, loss=11.1]\n",
      "[Epoch 004]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 107.25it/s, loss=9.73]\n",
      "[Epoch 005]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 110.58it/s, loss=8.57]\n",
      "[Epoch 006]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 109.62it/s, loss=12.9]\n",
      "[Epoch 007]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 108.70it/s, loss=7.32]\n",
      "[Epoch 008]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 107.38it/s, loss=9.96]\n",
      "[Epoch 009]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 107.24it/s, loss=6.28]\n",
      "[Epoch 010]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 104.16it/s, loss=7.97]\n",
      "[Epoch 011]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 106.10it/s, loss=4.94]\n",
      "[Epoch 012]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 106.45it/s, loss=8.71]\n",
      "[Epoch 013]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 106.40it/s, loss=3.72]\n",
      "[Epoch 014]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.99it/s, loss=4.82]\n",
      "[Epoch 015]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.38it/s, loss=4.02]\n",
      "[Epoch 016]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.01it/s, loss=6.18]\n",
      "[Epoch 017]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.47it/s, loss=4.29]\n",
      "[Epoch 018]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.05it/s, loss=6.19]\n",
      "[Epoch 019]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 104.69it/s, loss=3.24]\n",
      "[Epoch 020]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.90it/s, loss=3.66]\n",
      "[Epoch 021]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.13it/s, loss=3.58]\n",
      "[Epoch 022]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 106.08it/s, loss=5.38]\n",
      "[Epoch 023]: 100%|██████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.59it/s, loss=2.5]\n",
      "[Epoch 024]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 104.92it/s, loss=7.77]\n",
      "[Epoch 025]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 106.21it/s, loss=4.92]\n",
      "[Epoch 026]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 106.19it/s, loss=2.65]\n",
      "[Epoch 027]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.01it/s, loss=4.72]\n",
      "[Epoch 028]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 103.92it/s, loss=2.27]\n",
      "[Epoch 029]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.67it/s, loss=9.95]\n",
      "[Epoch 030]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 104.55it/s, loss=3.47]\n",
      "[Epoch 031]: 100%|████████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.83it/s, loss=6]\n",
      "[Epoch 032]: 100%|██████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.15it/s, loss=9.2]\n",
      "[Epoch 033]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.61it/s, loss=3.02]\n",
      "[Epoch 034]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 104.38it/s, loss=11.7]\n",
      "[Epoch 035]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 104.53it/s, loss=2.36]\n",
      "[Epoch 036]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 104.82it/s, loss=4.66]\n",
      "[Epoch 037]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 106.12it/s, loss=3.58]\n",
      "[Epoch 038]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 106.05it/s, loss=5.22]\n",
      "[Epoch 039]: 100%|████████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 106.12it/s, loss=3]\n",
      "[Epoch 040]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.18it/s, loss=4.78]\n",
      "[Epoch 041]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 104.84it/s, loss=5.84]\n",
      "[Epoch 042]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.86it/s, loss=2.91]\n",
      "[Epoch 043]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.07it/s, loss=3.21]\n",
      "[Epoch 044]: 100%|██████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 104.25it/s, loss=2.6]\n",
      "[Epoch 045]: 100%|██████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 104.73it/s, loss=4.3]\n",
      "[Epoch 046]: 100%|██████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 104.52it/s, loss=3.1]\n",
      "[Epoch 047]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.49it/s, loss=8.17]\n",
      "[Epoch 048]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 104.74it/s, loss=4.66]\n",
      "[Epoch 049]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.22it/s, loss=2.41]\n",
      "[Epoch 050]: 100%|█████████████████████████████████████████████████████| 1242/1242 [00:11<00:00, 105.73it/s, loss=3.11]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "from daisy.utils.data import PairData\n",
    "\n",
    "train_dataset = PairData(neg_set, is_training=True)\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=256, \n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")\n",
    "model.fit(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we need to build candidates set with ground truth in order to calculate the further metrics. Here we set candidates for each user in test set 1000 as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T08:05:56.059386Z",
     "start_time": "2020-08-20T08:05:55.516410Z"
    }
   },
   "outputs": [],
   "source": [
    "from daisy.utils.loader import build_candidates_set\n",
    "\n",
    "item_pool = set(range(item_num))\n",
    "candidates_num = 1000\n",
    "test_ucands = build_candidates_set(test_ur, total_train_ur, item_pool, candidates_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T08:12:22.266145Z",
     "start_time": "2020-08-20T08:11:18.450500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 941/941 [01:03<00:00, 14.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "preds = {}\n",
    "topk = 10\n",
    "for u in tqdm(test_ucands.keys()):\n",
    "    tmp = pd.DataFrame({\n",
    "        'user': [u for _ in test_ucands[u]], \n",
    "        'item': test_ucands[u], \n",
    "        'rating': [0. for _ in test_ucands[u]], # fake label, make nonsense\n",
    "    })\n",
    "    \n",
    "    tmp_neg_set = sampler.transform(tmp, is_training=False)\n",
    "    tmp_dataset = PairData(tmp_neg_set, is_training=False)\n",
    "    tmp_loader = data.DataLoader(\n",
    "        tmp_dataset,\n",
    "        batch_size=candidates_num, \n",
    "        shuffle=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    for items in tmp_loader:\n",
    "        user_u, item_i = items[0], items[1]\n",
    "        user_u = user_u.cpu()\n",
    "        item_i = item_i.cpu()\n",
    "        \n",
    "        prediction = model.predict(user_u, item_i)\n",
    "        \n",
    "        _, indices = torch.topk(prediction, topk)\n",
    "        top_n = torch.take(torch.tensor(test_ucands[u]), indices).cpu().numpy()\n",
    "        \n",
    "    preds[u] = top_n\n",
    "    \n",
    "# convert rank list to binary-interaction\n",
    "for u in preds.keys():\n",
    "    preds[u] = [1 if i in test_ur[u] else 0 for i in preds[u]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T08:14:30.288745Z",
     "start_time": "2020-08-20T08:14:30.157097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.2914\n",
      "Recall@10: 0.2034\n",
      "HR@10: 0.8959\n",
      "MAP@10: 0.1814\n",
      "MRR@10: 0.9728\n",
      "NDCG@10: 0.6157\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from daisy.utils.metrics import precision_at_k, recall_at_k, map_at_k, hr_at_k, ndcg_at_k, mrr_at_k\n",
    "\n",
    "tmp_preds = preds.copy()        \n",
    "tmp_preds = {key: rank_list[:topk] for key, rank_list in tmp_preds.items()}\n",
    "\n",
    "pre_k = np.mean([precision_at_k(r, topk) for r in tmp_preds.values()])\n",
    "rec_k = recall_at_k(tmp_preds, test_ur, topk)\n",
    "hr_k = hr_at_k(tmp_preds, test_ur)\n",
    "map_k = map_at_k(tmp_preds.values())\n",
    "mrr_k = mrr_at_k(tmp_preds, topk)\n",
    "ndcg_k = np.mean([ndcg_at_k(r, topk) for r in tmp_preds.values()])\n",
    "\n",
    "\n",
    "print(f'Precision@{topk}: {pre_k:.4f}')\n",
    "print(f'Recall@{topk}: {rec_k:.4f}')\n",
    "print(f'HR@{topk}: {hr_k:.4f}')\n",
    "print(f'MAP@{topk}: {map_k:.4f}')\n",
    "print(f'MRR@{topk}: {mrr_k:.4f}')\n",
    "print(f'NDCG@{topk}: {ndcg_k:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OverView\n",
    "\n",
    "All codes above have been wrapped in `main.py`. It is equal to the following instruction. You can get the same result in command console with this method.\n",
    "\n",
    "\n",
    "```\n",
    "python main.py --problem_type=pair --algo_name=mf --loss_type=BPR --num_ng=4 --lr=0.01 --reg_1=0 --reg_2=0.01 --factors=15 --epochs=50\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
